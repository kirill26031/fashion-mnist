{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 16:36:28,043\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-10-08 16:36:28,855\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "from ray import tune\n",
    "from ray.train import Checkpoint, get_checkpoint, report\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate_dataset = FashionMNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = FashionMNIST(root='data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_train = pd.read_csv('data/fashion-mnist_train.csv')\n",
    "csv_test = pd.read_csv('data/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classes:\n",
    "classes = train_and_validate_dataset.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataframe is class (from 0 to 9) and 28*28 monohrome pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of dataframes in train, test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size:\", len(csv_train))\n",
    "print(\"Test dataset size:\", len(csv_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(dataset: FashionMNIST, index: int):\n",
    "    figure = plt.figure(figsize = (5,5))\n",
    "    image, label = dataset[index]\n",
    "    plt.title(dataset.classes[label])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example of image\n",
    "show_image(train_and_validate_dataset, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_distribution(frames: pd.DataFrame, classes: list[str]):\n",
    "    value_counts = frames['label'].value_counts()\n",
    "    total_amount = len(frames)\n",
    "    for i in range(len(value_counts)):\n",
    "        label = classes[value_counts.index[i]]\n",
    "        amount = value_counts.values[i]\n",
    "        percentage = amount * 100 / total_amount\n",
    "        print(\"#{} {:<15s}:   {} or {}%\".format(value_counts.index[i], label, amount, percentage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set class distribution\n",
      "#2 Pullover       :   6000 or 10.0%\n",
      "#9 Ankle boot     :   6000 or 10.0%\n",
      "#6 Shirt          :   6000 or 10.0%\n",
      "#0 T-shirt/top    :   6000 or 10.0%\n",
      "#3 Dress          :   6000 or 10.0%\n",
      "#4 Coat           :   6000 or 10.0%\n",
      "#5 Sandal         :   6000 or 10.0%\n",
      "#8 Bag            :   6000 or 10.0%\n",
      "#7 Sneaker        :   6000 or 10.0%\n",
      "#1 Trouser        :   6000 or 10.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set class distribution\")\n",
    "get_classes_distribution(csv_train, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set class distribution\n",
      "#0 T-shirt/top    :   1000 or 10.0%\n",
      "#1 Trouser        :   1000 or 10.0%\n",
      "#2 Pullover       :   1000 or 10.0%\n",
      "#3 Dress          :   1000 or 10.0%\n",
      "#8 Bag            :   1000 or 10.0%\n",
      "#6 Shirt          :   1000 or 10.0%\n",
      "#5 Sandal         :   1000 or 10.0%\n",
      "#4 Coat           :   1000 or 10.0%\n",
      "#7 Sneaker        :   1000 or 10.0%\n",
      "#9 Ankle boot     :   1000 or 10.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set class distribution\")\n",
    "get_classes_distribution(csv_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv_layers(conv1_out, conv1_kernel_size, conv2_out, conv2_kernel_size):\n",
    "    conv2_in = conv1_out \n",
    "    return nn.ParameterList([\n",
    "        nn.Conv2d(in_channels=1, out_channels=conv1_out, kernel_size=conv1_kernel_size),\n",
    "        nn.Conv2d(in_channels=conv2_in, out_channels=conv2_out, kernel_size=conv2_kernel_size)\n",
    "    ])\n",
    "\n",
    "def construct_fc_layers(after_conv_size, fc1_out, fc2_out, classes_amount):\n",
    "    return nn.ParameterList([\n",
    "        nn.Linear(in_features=after_conv_size, out_features=fc1_out),\n",
    "        nn.Linear(in_features=fc1_out, out_features=fc2_out),\n",
    "        nn.Linear(in_features=fc2_out, out_features=classes_amount)\n",
    "    ])\n",
    "\n",
    "def kernel_size_reduction(kernel_size: int):\n",
    "    return (kernel_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module): \n",
    "    def __init__(self, conv1_out, conv1_kernel_size, conv2_out, conv2_kernel_size, fc1_out, fc2_out, image_size = 28, classes_amount = 10):\n",
    "        super().__init__()\n",
    "        self.conv_layers = construct_conv_layers(conv1_out, conv1_kernel_size, conv2_out, conv2_kernel_size)\n",
    "        self.activation = nn.functional.relu\n",
    "        self.after_conv_size = (image_size - kernel_size_reduction(conv1_kernel_size) - kernel_size_reduction(conv2_kernel_size))**2 * conv2_out\n",
    "        self.pool = nn.functional.adaptive_avg_pool2d\n",
    "        self.fully_connected_layers = construct_fc_layers(self.after_conv_size, fc1_out, fc2_out, classes_amount)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            conv_l = self.conv_layers[i]\n",
    "            x = conv_l(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.pool(x, output_size=x.shape[2])\n",
    "        \n",
    "        x = x.view(-1, self.after_conv_size)\n",
    "        \n",
    "        for i in range(len(self.fully_connected_layers)):\n",
    "            fc_l = self.fully_connected_layers[i]\n",
    "            x = fc_l(x)\n",
    "            if i + 1 != len(self.fully_connected_layers):\n",
    "                x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, loader, func):\n",
    "        self.loader = loader\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in iter(self.loader):\n",
    "            yield self.func(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def to_device(X, y):\n",
    "    return X.to(device), y.to(device, dtype=torch.int64)\n",
    "\n",
    "train_data, validate_data = random_split(dataset=train_and_validate_dataset, lengths=[0.8, 0.2])\n",
    "test_data = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, loss_func, X, y, optimizer=None):\n",
    "    loss_ = loss_func(model(X), y)\n",
    "    if optimizer is not None:\n",
    "      loss_.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    return loss_.item(), len(X)\n",
    "\n",
    "def validate(model, loss_func, X, y):\n",
    "    output = model(X)\n",
    "    loss_ = loss_func(output, y)\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    correct = pred == y.view(*pred.shape)\n",
    "\n",
    "    return loss_.item(), torch.sum(correct).item(), len(X)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    accuracy_by_class = np.zeros(10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        length = 0\n",
    "\n",
    "        for x, y in loader:\n",
    "            loss_, correct_, len_ = validate(model, nn.CrossEntropyLoss(), x, y)\n",
    "            losses.append(np.multiply(loss_, len_))\n",
    "            correct += correct_\n",
    "            # works only with batch_size = 0\n",
    "            accuracy_by_class[y[0].item()] += correct_\n",
    "            length += len_\n",
    "        test_loss = np.sum(losses) / length\n",
    "        test_accuracy = correct / length * 100\n",
    "\n",
    "        print(f\"Test loss: {test_loss:.5f}\\t\"\n",
    "          f\"Test accuracy: {test_accuracy:.3f}%\")\n",
    "        print(accuracy_by_class)\n",
    "        return accuracy_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, report_func=report): \n",
    "    conv1_out = config[\"conv2_out\"] if config[\"conv1_out\"] < config[\"conv2_out\"] else config[\"conv1_out\"]\n",
    "    conv2_out = config[\"conv1_out\"] if config[\"conv1_out\"] < config[\"conv2_out\"] else config[\"conv2_out\"]\n",
    "    fc1_out = config[\"fc2_out\"] if config[\"fc1_out\"] < config[\"fc2_out\"] else config[\"fc1_out\"]\n",
    "    fc2_out = config[\"fc1_out\"] if config[\"fc1_out\"] < config[\"fc2_out\"] else config[\"fc2_out\"]\n",
    "\n",
    "    if (28 - config[\"conv1_kernel_size\"] - config[\"conv2_kernel_size\"] + 2) >= fc1_out:\n",
    "        raise BaseException(\"incorrect params\")\n",
    "    \n",
    "    model = MLP(conv1_out=conv1_out, conv1_kernel_size=config[\"conv1_kernel_size\"],\n",
    "                conv2_out=conv2_out, conv2_kernel_size=config[\"conv2_kernel_size\"],\n",
    "                fc1_out=fc1_out, fc2_out=fc2_out).to(device=device)\n",
    "    patience = 5\n",
    "    epochs = 40\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config[\"lr\"])\n",
    "\n",
    "    train_loader = WrappedDataLoader(DataLoader(train_data, batch_size = int(config[\"batch_size\"])), to_device)\n",
    "    validate_loader = WrappedDataLoader(DataLoader(validate_data, batch_size = int(config[\"batch_size\"])), to_device)\n",
    "    \n",
    "    loss_min = np.Inf\n",
    "    wait = 0\n",
    "\n",
    "    if get_checkpoint():\n",
    "        loaded_checkpoint = get_checkpoint()\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(\n",
    "                os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\")\n",
    "            )\n",
    "            model.load_state_dict(model_state)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        losses = []\n",
    "        length = 0\n",
    "        for tr_x, tr_y in train_loader:\n",
    "            loss_, len_ = loss(model, nn.CrossEntropyLoss(), tr_x, tr_y, optimizer)\n",
    "            losses.append(np.multiply(loss_, len_))\n",
    "            length += len_\n",
    "        train_loss = np.sum(losses) / length\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad(), tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "\n",
    "            losses = []\n",
    "            correct = 0\n",
    "            length = 0\n",
    "            for v_x, v_y in validate_loader:\n",
    "                loss_, correct_, len_ = validate(model=model, loss_func=nn.CrossEntropyLoss(), X=v_x, y=v_y)\n",
    "                losses.append(np.multiply(loss_, len_))\n",
    "                correct += correct_\n",
    "                length += len_\n",
    "            valid_loss = sum(losses) / length\n",
    "            valid_accuracy = correct / length\n",
    "\n",
    "            print(f\"\\nepoch: {epoch+1:3}, loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, valid accuracy: {(valid_accuracy * 100):.3f}%\")\n",
    "\n",
    "            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(\n",
    "                (model.state_dict(), optimizer.state_dict()), path\n",
    "            )\n",
    "            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "            report_func(\n",
    "                {\"loss\": valid_loss, \"accuracy\": valid_accuracy},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "\n",
    "            # Save model if validation loss has decreased\n",
    "            if valid_loss <= loss_min:\n",
    "                # print(f\"Validation loss decreased ({loss_min:.6f} --> {valid_loss:.6f}). Saving model...\")\n",
    "                # torch.save(model.state_dict(), model_name + '.pt')\n",
    "                loss_min = valid_loss\n",
    "                wait = 0\n",
    "            # Early stopping\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    print(f\"Terminated Training for Early Stopping at Epoch {epoch+1}\")\n",
    "                    return\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_scope = [2, 4, 8, 16, 32]\n",
    "kernel_size_scope = [3, 5]\n",
    "conv_size_scope = [4, 8, 16, 32, 64, 128]\n",
    "fc_size_scope = [16, 32, 64, 128]\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"batch_size\": tune.choice(batch_size_scope),\n",
    "    \"conv1_kernel_size\": tune.choice(kernel_size_scope),\n",
    "    \"conv2_kernel_size\": tune.choice(kernel_size_scope),\n",
    "    \"conv1_out\": tune.choice(conv_size_scope),\n",
    "    \"conv2_out\": tune.choice(conv_size_scope),\n",
    "    \"fc1_out\": tune.choice(fc_size_scope),\n",
    "    \"fc2_out\": tune.choice(fc_size_scope)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(checkpoint_dir, config):\n",
    "    best_trained_model = MLP(conv1_out=config[\"conv1_out\"], conv1_kernel_size=config[\"conv1_kernel_size\"],\n",
    "                conv2_out=config[\"conv2_out\"], conv2_kernel_size=config[\"conv2_kernel_size\"],\n",
    "                fc1_out=config[\"fc1_out\"], fc2_out=config[\"fc2_out\"]).to(device=device)\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n",
    "\n",
    "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_loader = WrappedDataLoader(DataLoader(test_data, batch_size = 1), to_device)\n",
    "    return evaluate(model=best_trained_model, loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 10-08 16:36:41] ax.service.utils.with_db_settings_base: Ax currently requires a sqlalchemy version below 2.0. This will be addressed in a future release. Disabling SQL storage in Ax for now, if you would like to use SQL storage please install Ax with mysql extras via `pip install ax-platform[mysql]`.\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.search.ax import AxSearch\n",
    "\n",
    "def train_model(num_samples=128, max_num_epochs=40, gpus_per_trial=0.125):\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    ax_search = AxSearch(parameter_constraints=[\n",
    "        \"conv1_out >= conv2_out\",\n",
    "        \"28 - conv1_kernel_size - conv2_kernel_size + 2 >= fc1_out\",\n",
    "        \"fc1_out >= fc2_out\",\n",
    "        \"fc2_out >= 10\"\n",
    "    ])\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train),\n",
    "            resources={\"cpu\": 0.5, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "            # search_alg=ax_search\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "    test_best_model(config=best_result.config, checkpoint_dir=best_result.checkpoint.to_directory())\n",
    "\n",
    "    # check_point = torch.load(model_name + '.pt', map_location=device)\n",
    "    # model.load_state_dict(check_point)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = {\n",
    "    \"lr\": 0.00031588,\n",
    "    \"batch_size\": 16,\n",
    "    \"conv1_kernel_size\": 3,\n",
    "    \"conv2_kernel_size\": 3,\n",
    "    \"conv1_out\": 64,\n",
    "    \"conv2_out\": 32,\n",
    "    \"fc1_out\": 64,\n",
    "    \"fc2_out\": 32\n",
    "}\n",
    "\n",
    "accuracy_by_label = test_best_model(checkpoint_dir=\"C:\\\\Users\\\\pc\\\\ray_results\\\\train_2024-10-08_16-42-18\\\\train_2354a_00114_114_batch_size=16,conv1_kernel_size=3,conv1_out=64,conv2_kernel_size=3,conv2_out=32,fc1_out=32,fc2_out=64,lr=0.0_2024-10-08_16-45-09\\\\checkpoint_000003\", config = best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt/top:   81.8%\n",
      "Trouser:   97.1%\n",
      "Pullover:   87.5%\n",
      "Dress:   87.1%\n",
      "Coat:   88.4%\n",
      "Sandal:   97.7%\n",
      "Shirt:   61.0%\n",
      "Sneaker:   95.8%\n",
      "Bag:   98.3%\n",
      "Ankle boot:   94.5%\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(accuracy_by_label)):\n",
    "    print(f\"{classes[i]}:   {accuracy_by_label[i]/10}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model recognizes bag, trousers, sandals the best.\n",
    "Model recognizes shirts the worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "def my_report(metrics: dict, checkpoint: Checkpoint):\n",
    "    loss = metrics[\"loss\"]\n",
    "    accuracy = metrics[\"accuracy\"]\n",
    "    runs.append({\"loss\": loss, \"accuracy\": accuracy, \"checkpoint\": checkpoint})\n",
    "\n",
    "def explore_adjacent_to_best(best_config, variable_parameter: str, scope: list):\n",
    "    best_runs = []\n",
    "    for i in range(len(scope)):\n",
    "        new_config = best_config.copy()\n",
    "        new_config[variable_parameter] = scope[i]\n",
    "        train(new_config, report_func=my_report)\n",
    "        best_runs.append(runs[-1])\n",
    "    return best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_runs = explore_adjacent_to_best(best_config, \"conv1_out\", conv_size_scope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
